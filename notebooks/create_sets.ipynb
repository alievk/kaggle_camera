{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import exifread\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('../data')\n",
    "train_root = data_root / 'train_orig'\n",
    "flickr_root = data_root / 'external/flickr_images'\n",
    "ext_valid_root = data_root / 'external/val_images'\n",
    "sets_root = data_root / 'sets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation splits from the official dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "train_set = []\n",
    "valid_set = []\n",
    "\n",
    "fix_path = lambda p: p.relative_to(train_root)\n",
    "\n",
    "for class_dir in train_root.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "    print(class_dir)\n",
    "    image_paths = [path for path in class_dir.glob('*.jpg')]\n",
    "    image_paths.extend([path for path in class_dir.glob('*.JPG')])\n",
    "    n_images = len(image_paths)\n",
    "    assert n_images == 275, n_images\n",
    "    \n",
    "    image_paths = [fix_path(p) for p in image_paths]\n",
    "    \n",
    "    image_paths = np.random.permutation(image_paths)\n",
    "    train_image_paths = image_paths[:int(n_images * train_split)]\n",
    "    valid_image_paths = image_paths[int(n_images * train_split):]\n",
    "    \n",
    "    train_set.extend(train_image_paths)\n",
    "    valid_set.extend(valid_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'fname': train_set}).to_csv(str(sets_root / 'train.csv'), index=None)\n",
    "pd.DataFrame({'fname': valid_set}).to_csv(str(sets_root / 'valid.csv'), index=None)\n",
    "pd.DataFrame({'fname': train_set + valid_set}).to_csv(str(sets_root / 'trainval.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(str(sets_root / 'train.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create validation dataset from the FLICKR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(exif_per_model, prop, unique=True):\n",
    "    out = {}\n",
    "    for model, exifs in exif_per_model.items():\n",
    "        props = [exif[prop].__str__() for _, exif in exifs if prop in exif]\n",
    "        out[model] = set(props) if unique else props\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exif_per_model = pickle.load(open(str(flickr_root / 'exif_per_model.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_paths = []\n",
    "with open(str(flickr_root / 'good_jpgs_andres')) as f:\n",
    "    for path in [l.strip() for l in f.readlines()]:\n",
    "        if not (flickr_root/Path(path)).exists():\n",
    "            print('{} not found'.format(path))\n",
    "        else:\n",
    "            flickr_paths.append(path)\n",
    "\n",
    "with open(str(flickr_root / 'good_jpgs'), 'w') as f:\n",
    "    for p in flickr_paths:\n",
    "        f.write(str(p) + '\\n')\n",
    "\n",
    "flickr_models, count = np.unique([Path(p).parts[0] for p in flickr_paths], return_counts=True)\n",
    "[m for m in zip(list(flickr_models), list(count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_quality = []\n",
    "with open(str(flickr_root / 'low-quality.txt')) as f:\n",
    "    for path in [l.strip() for l in f.readlines()]:\n",
    "        low_quality.append(path.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_train_split = 0.8\n",
    "flickr_max_model_samples = min(count)\n",
    "print('max samples', flickr_max_model_samples)\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "flickr_train_set = []\n",
    "flickr_valid_set = []\n",
    "\n",
    "#flickr_fix_path = lambda p: (flickr_root / p).relative_to('..')\n",
    "\n",
    "flickr_paths_m = defaultdict(list)\n",
    "for path in flickr_paths:\n",
    "    m = name_map[Path(path).parts[0]]\n",
    "    flickr_paths_m[m].append(path)\n",
    "\n",
    "for m, paths in flickr_paths_m.items():\n",
    "    paths = np.random.permutation(paths)[:min(flickr_max_model_samples, len(paths))]\n",
    "    n_images = len(paths)\n",
    "    train_paths = paths[:int(n_images * flickr_train_split)]\n",
    "    valid_paths = paths[int(n_images * flickr_train_split):]\n",
    "    flickr_train_set.extend(train_paths)\n",
    "    flickr_valid_set.extend(valid_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manip = [int(p in low_quality) for p in flickr_train_set]\n",
    "valid_manip = [int(p in low_quality) for p in flickr_valid_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['fname', 'manip']\n",
    "pd.DataFrame(dict(zip(columns, [flickr_train_set, train_manip])), columns=columns).to_csv(\n",
    "    str(sets_root / 'flickr_train.csv'), index=None)\n",
    "pd.DataFrame(dict(zip(columns, [flickr_valid_set, valid_manip])), columns=columns).to_csv(\n",
    "    str(sets_root / 'flickr_valid.csv'), index=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract EXIF and save\n",
    "\n",
    "exif_per_model = defaultdict(list)\n",
    "for model in flickr_root.iterdir():\n",
    "    if not model.is_dir():\n",
    "        continue\n",
    "    print(model)\n",
    "    for p in model.iterdir():\n",
    "        if '.jpg' not in str(p) and '.JPG' not in str(p):\n",
    "            print('skip', str(p))\n",
    "            continue\n",
    "        with open(str(p), 'rb') as fh:\n",
    "            _tags = exifread.process_file(fh)\n",
    "            tags = {k: v for k, v in _tags.items() if 'thumbnail' not in k.lower()}\n",
    "            exif_per_model[str(model.stem)].append((str(p), tags))\n",
    "            \n",
    "pickle.dump(exif_per_model, open(str(flickr_root / 'exif_per_model.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "orig_img_sizes = pickle.load(open(str(train_root / 'img_sizes.pkl'), 'rb'))\n",
    "\n",
    "ok_sized = defaultdict(list)\n",
    "\n",
    "# -> it's not reliable to read image size from EXIF\n",
    "\n",
    "# for _model, exifs in exif_per_model.items():\n",
    "#     for path, exif in exifs:\n",
    "#         if 'EXIF ExifImageWidth' not in exif.keys() or 'EXIF ExifImageLength' not in exif.keys():\n",
    "#             sz = list(cv2.imread(str(path)).shape[:2])\n",
    "#         else:\n",
    "#             sz = [int(exif['EXIF ExifImageWidth'].__str__()), int(exif['EXIF ExifImageLength'].__str__())]\n",
    "#         model = name_map[_model]\n",
    "#         if sz in orig_img_sizes[model]:\n",
    "#             ok_sized[model].append(path)\n",
    "\n",
    "# read image size with opencv\n",
    "for model_dir in flickr_root.iterdir():\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    print(model_dir)\n",
    "    model = name_map[model_dir.stem]\n",
    "    imgs = [p for p in model_dir.glob('*.jpg')]\n",
    "    imgs.extend([p for p in model_dir.glob('*.JPG')])\n",
    "    for p in imgs:\n",
    "        sz = cv2.imread(str(p)).shape[:2]\n",
    "        if list(sz) in orig_img_sizes[model]:\n",
    "            ok_sized[model].append(p)\n",
    "    print('{}: {}/{}'.format(model, len(ok_sized[model]), len(imgs)))\n",
    "    \n",
    "with open(str(flickr_root / 'good_jpegs_my.txt'), 'w') as fh:\n",
    "for paths in ok_sized.values():\n",
    "    for p in paths:\n",
    "        fh.write(str(p.relative_to(flickr_root))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_make = get_props(exif_per_model, 'Image Make', unique=False)\n",
    "np.unique(im_make['nexus_5x'],return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_camera_env",
   "language": "python",
   "name": "kaggle_camera_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
